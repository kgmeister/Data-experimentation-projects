import os, re, time, requests
from typing import Dict, List, Optional, Tuple

_NUM_URL_KEY = re.compile(r"^LLM_(\d+)_API_URL$", re.I)

def _parse_status_list(spec: str) -> List[Tuple[int, int]]:
    out = []
    for part in (spec or "").split(","):
        part = part.strip()
        if not part:
            continue
        if "-" in part:
            a, b = part.split("-", 1)
            out.append((int(a), int(b)))
        else:
            v = int(part)
            out.append((v, v))
    return out

def _status_matches(ranges: List[Tuple[int, int]], status: int) -> bool:
    return any(lo <= status <= hi for lo, hi in ranges)

def _parse_extra_headers(raw: Optional[str]) -> Dict[str, str]:
    """
    'Key1: Val1; Key2: Val2' -> {'Key1': 'Val1', 'Key2': 'Val2'}
    """
    headers = {}
    if not raw:
        return headers
    parts = [p.strip() for p in raw.split(";") if p.strip()]
    for p in parts:
        if ":" in p:
            k, v = p.split(":", 1)
            headers[k.strip()] = v.strip()
    return headers

def _discover_indices(env: Dict[str, str]) -> List[int]:
    """
    Find Ns that have LLM_N_API_URL (and likely other fields).
    Also add a virtual 0 if legacy API_URL/API_KEY present.
    """
    nums = set()
    for k in env.keys():
        m = _NUM_URL_KEY.match(k)
        if m:
            nums.add(int(m.group(1)))
    if (env.get("API_URL") or env.get("LLM_API_URL")) and (env.get("API_KEY") or env.get("LLM_API_KEY")):
        nums.add(0)  # legacy block
    return sorted(nums)

def _provider_from_env(env: Dict[str, str], n: int) -> Optional[Dict[str, str]]:
    """
    Build a provider dict with keys:
      idx, api_url, api_key, model (optional), auth_type, extra_headers
    Returns None if required pieces are missing.
    """
    if n == 0:
        api_url = env.get("LLM_API_URL") or env.get("API_URL")
        api_key = env.get("LLM_API_KEY") or env.get("API_KEY")
        model   = env.get("LLM_MODEL")   or env.get("MODEL") or ""
        auth    = (env.get("LLM_AUTH_TYPE") or "bearer").strip().lower()
        extra   = env.get("LLM_HEADERS") or ""
    else:
        p = f"LLM_{n}_"
        api_url = env.get(p + "API_URL")
        api_key = env.get(p + "API_KEY")
        model   = (env.get(p + "MODEL") or "").strip()
        auth    = (env.get(p + "AUTH_TYPE") or "bearer").strip().lower()
        extra   = env.get(p + "HEADERS") or ""

    if not (api_url and api_key):
        return None

    return {
        "idx": n,
        "api_url": api_url.strip(),
        "api_key": api_key.strip(),
        "model": model,
        "auth_type": auth,
        "extra_headers": extra,
        "name": f"LLM_{n}",
    }

def _auth_headers(auth_type: str, key: str) -> Dict[str, str]:
    """
    bearer        -> Authorization: Bearer <key>
    api-key       -> api-key: <key>
    header:Name   -> Name: <key>
    """
    headers = {}
    if auth_type == "bearer":
        headers["Authorization"] = f"Bearer {key}"
    elif auth_type == "api-key":
        headers["api-key"] = key
    elif auth_type.startswith("header:"):
        name = auth_type.split(":", 1)[1].strip()
        if name:
            headers[name] = key
    else:
        # default to bearer for unknown values
        headers["Authorization"] = f"Bearer {key}"
    return headers

def iter_llm_providers() -> List[Dict[str, str]]:
    env = os.environ
    providers: List[Dict[str, str]] = []
    for n in _discover_indices(env):
        p = _provider_from_env(env, n)
        if p:
            providers.append(p)
    return providers

def chat_with_failover(messages,
                       temperature: Optional[float] = None,
                       max_tokens: Optional[int] = None) -> str:
    """
    Tries LLM_1, LLM_2, LLM_3, ... sequentially until one works.
    Supports OpenAI-compatible and Azure chat-completions endpoints,
    as long as you supply the full API_URL and the right AUTH_TYPE.
    """
    timeout  = float(os.getenv("LLM_TIMEOUT_SECONDS", "25"))
    retries  = int(os.getenv("LLM_RETRIES", "2"))
    backoff  = float(os.getenv("LLM_BACKOFF_SECONDS", "1.2"))
    failover_ranges = _parse_status_list(os.getenv("LLM_FAILOVER_ON_STATUS", "500-599"))

    if temperature is None:
        temperature = float(os.getenv("LLM_DEFAULT_TEMPERATURE", "0.2"))
    if max_tokens is None:
        mt_env = os.getenv("LLM_DEFAULT_MAX_TOKENS")
        max_tokens = int(mt_env) if mt_env else None

    last_err: Optional[Exception] = None

    for p in iter_llm_providers():
        base_headers = {
            "Content-Type": "application/json",
            **_auth_headers(p["auth_type"], p["api_key"]),
            **_parse_extra_headers(p["extra_headers"]),
        }

        for attempt in range(retries + 1):
            try:
                payload = {"messages": messages, "temperature": temperature}
                if p["model"]:
                    payload["model"] = p["model"]
                if max_tokens is not None:
                    payload["max_tokens"] = max_tokens

                r = requests.post(p["api_url"], json=payload, headers=base_headers, timeout=timeout)

                if r.status_code == 200:
                    data = r.json()
                    # Expect OpenAI/Azure-style response:
                    # { choices: [ { message: { content: "..." } } ] }
                    choices = data.get("choices") or []
                    if choices and "message" in choices[0] and "content" in choices[0]["message"]:
                        return choices[0]["message"]["content"]
                    # Fallback: some providers may return slightly different shapes
                    # Try the common alternative 'text' field
                    if choices and "text" in choices[0]:
                        return choices[0]["text"]
                    raise RuntimeError(f"{p['name']} unexpected response: {str(data)[:400]}")

                # Decide failover or retry
                if _status_matches(failover_ranges, r.status_code):
                    last_err = RuntimeError(f"{p['name']} HTTP {r.status_code}: {r.text[:400]}")
                    break  # move to next provider
                else:
                    last_err = RuntimeError(f"{p['name']} HTTP {r.status_code}: {r.text[:400]}")
                    time.sleep(backoff * (attempt + 1))  # retry same provider

            except requests.RequestException as e:
                last_err = e
                time.sleep(backoff * (attempt + 1))  # retry same provider

        # try next provider

    raise last_err or RuntimeError("All LLM providers failed")

# --- Example:
# messages = [
#     {"role": "system", "content": "You are a helpful assistant."},
#     {"role": "user", "content": "Hello!"},
# ]
# print(chat_with_failover(messages))
