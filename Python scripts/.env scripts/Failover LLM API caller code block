# llm_failover.py
# Replace env loaders in script with this
# =============================================================================
# LLM provider failover with numbered .env keys and legacy primary support.
# - Tries legacy (API_URL/API_KEY/MODEL) as LLM_0 if present.
# - Then tries LLM_1, LLM_2, LLM_3, ... in ascending order.
# - No carry-over of keys/models between providers (fresh request per attempt).
# - Supports OpenAI-style and Azure chat-completions (via API_URL + AUTH_TYPE).
#
# Env knobs (typical):
#   LLM_TIMEOUT_SECONDS=25
#   LLM_RETRIES=2
#   LLM_BACKOFF_SECONDS=1.2
#   LLM_FAILOVER_ON_STATUS=401,403,404,408,429,500-599
#   LLM_DEFAULT_TEMPERATURE=0.2
#   LLM_DEFAULT_MAX_TOKENS=1024
#   LLM_FAILOVER_DEBUG=0|1   (optional debug prints)
#
# Per provider (numbered):
#   LLM_N_API_URL=...
#   LLM_N_API_KEY=...
#   LLM_N_MODEL=...          (optional for providers that don't need model)
#   LLM_N_AUTH_TYPE=bearer|api-key|header:Name
#   LLM_N_HEADERS="K1: V1; K2: V2"  (optional extra headers)
#
# Legacy (optional, treated as LLM_0 if API_URL and API_KEY are set):
#   API_URL=...
#   API_KEY=...
#   MODEL=...
#   LLM_AUTH_TYPE=...        (auth type for legacy)
#   LLM_HEADERS=...          (extra headers for legacy)
# =============================================================================

# ── Section 1: Imports & discovery pattern ─────────────────────────────────────
import os, re, time, requests
from typing import Dict, List, Optional, Tuple

# Detect keys like LLM_1_API_URL, LLM_2_API_URL, ...
_NUM_URL_KEY = re.compile(r"^LLM_(\d+)_API_URL$", re.I)


# ── Section 2: Failover status parsing helpers ─────────────────────────────────
def _parse_status_list(spec: str) -> List[Tuple[int, int]]:
    """
    '401,403,404,408,429,500-599' -> [(401,401),(403,403),(404,404),(408,408),(429,429),(500,599)]
    Used to decide whether to switch provider immediately.
    """
    out = []
    for part in (spec or "").split(","):
        part = part.strip()
        if not part:
            continue
        if "-" in part:
            a, b = part.split("-", 1)
            out.append((int(a), int(b)))
        else:
            v = int(part)
            out.append((v, v))
    return out


def _status_matches(ranges: List[Tuple[int, int]], status: int) -> bool:
    """Return True if HTTP status falls inside any configured range."""
    return any(lo <= status <= hi for lo, hi in ranges)


# ── Section 3: Header utilities ────────────────────────────────────────────────
def _parse_extra_headers(raw: Optional[str]) -> Dict[str, str]:
    """
    Parse "Key1: Val1; Key2: Val2" into {"Key1": "Val1", "Key2": "Val2"}.
    Used for provider-specific custom headers.
    """
    headers: Dict[str, str] = {}
    if not raw:
        return headers
    for p in [p.strip() for p in raw.split(";") if p.strip()]:
        if ":" in p:
            k, v = p.split(":", 1)
            headers[k.strip()] = v.strip()
    return headers


def _auth_headers(auth_type: str, key: str) -> Dict[str, str]:
    """
    Map AUTH_TYPE → auth header(s):
      - bearer      → Authorization: Bearer <key>
      - api-key     → api-key: <key>   (e.g., Azure-style)
      - header:Name → Name: <key>      (escape hatch)
    """
    auth_type = (auth_type or "bearer").strip().lower()
    if auth_type == "bearer":
        return {"Authorization": f"Bearer {key}"}
    if auth_type == "api-key":
        return {"api-key": key}
    if auth_type.startswith("header:"):
        name = auth_type.split(":", 1)[1].strip() or "Authorization"
        return {name: key}
    # Default to bearer if unspecified/unknown:
    return {"Authorization": f"Bearer {key}"}


# ── Section 4: Provider discovery & normalization ──────────────────────────────
def _discover_indices(env: Dict[str, str]) -> List[int]:
    """
    Find all N that have LLM_N_API_URL.
    Also add a virtual 0 if legacy API_URL/API_KEY are present (treated as LLM_0).
    """
    nums = {int(m.group(1)) for k in env.keys() for m in [_NUM_URL_KEY.match(k)] if m}
    if (env.get("API_URL") or env.get("LLM_API_URL")) and (env.get("API_KEY") or env.get("LLM_API_KEY")):
        nums.add(0)  # legacy first
    return sorted(nums)


def _provider_from_env(env: Dict[str, str], n: int) -> Optional[Dict[str, str]]:
    """
    Build a normalized provider dict:
      { idx, name, api_url, api_key, model, auth_type, extra_headers }
    Returns None if required pieces are missing.
    """
    if n == 0:
        # Legacy primary (LLM_0)
        api_url = env.get("LLM_API_URL") or env.get("API_URL")
        api_key = env.get("LLM_API_KEY") or env.get("API_KEY")
        model   = (env.get("LLM_MODEL") or env.get("MODEL") or "").strip()
        auth    = (env.get("LLM_AUTH_TYPE") or "bearer").strip().lower()
        extra   = env.get("LLM_HEADERS") or ""
    else:
        p = f"LLM_{n}_"
        api_url = env.get(p + "API_URL")
        api_key = env.get(p + "API_KEY")
        model   = (env.get(p + "MODEL") or "").strip()
        auth    = (env.get(p + "AUTH_TYPE") or "bearer").strip().lower()
        extra   = env.get(p + "HEADERS") or ""

    if not (api_url and api_key):
        return None

    return {
        "idx": n,
        "name": f"LLM_{n}",
        "api_url": api_url.strip(),
        "api_key": api_key.strip(),
        "model": model,
        "auth_type": auth,
        "extra_headers": extra,
    }


def iter_llm_providers() -> List[Dict[str, str]]:
    """
    Return providers in ascending order:
      LLM_0 (legacy, if present), then LLM_1, LLM_2, ...
    """
    env = os.environ
    out: List[Dict[str, str]] = []
    for n in _discover_indices(env):
        p = _provider_from_env(env, n)
        if p:
            out.append(p)
    return out


# ── Section 5: Request builder (fresh objects per attempt) ─────────────────────
def _build_request(provider: Dict[str, str], messages, temperature: float, max_tokens: Optional[int]):
    """
    Always returns brand-new (url, headers, payload) so that NOTHING leaks
    across providers or attempts.
    """
    url = provider["api_url"]

    headers = {"Content-Type": "application/json"}
    headers.update(_auth_headers(provider["auth_type"], provider["api_key"]))
    headers.update(_parse_extra_headers(provider["extra_headers"]))

    payload = {"messages": messages, "temperature": temperature}
    if provider["model"]:
        payload["model"] = provider["model"]
    if max_tokens is not None:
        payload["max_tokens"] = max_tokens

    return url, headers, payload


# ── Section 6: Failover chat call ─────────────────────────────────────────────
def chat_with_failover(messages,
                       temperature: Optional[float] = None,
                       max_tokens: Optional[int] = None) -> str:
    """
    Tries LLM_0 (legacy) if present, then LLM_1, LLM_2, ... until one returns 200.
    - Retries each provider a few times (LLM_RETRIES) before moving on
    - Switches immediately on configured failover statuses
    - Compatible with OpenAI-style and Azure chat-completions responses
    Returns the assistant message content (string).
    """
    timeout  = float(os.getenv("LLM_TIMEOUT_SECONDS", "25"))
    retries  = int(os.getenv("LLM_RETRIES", "2"))
    backoff  = float(os.getenv("LLM_BACKOFF_SECONDS", "1.2"))
    failover_ranges = _parse_status_list(os.getenv("LLM_FAILOVER_ON_STATUS", "500-599"))
    debug = os.getenv("LLM_FAILOVER_DEBUG", "0") == "1"

    if temperature is None:
        temperature = float(os.getenv("LLM_DEFAULT_TEMPERATURE", "0.2"))
    if max_tokens is None:
        mt = os.getenv("LLM_DEFAULT_MAX_TOKENS")
        max_tokens = int(mt) if mt else None

    last_err: Optional[Exception] = None

    for p in iter_llm_providers():
        if debug:
            print(f"[llm_failover] Trying {p['name']} at {p['api_url']}")
        for attempt in range(retries + 1):
            try:
                url, headers, payload = _build_request(p, messages, temperature, max_tokens)
                r = requests.post(url, json=payload, headers=headers, timeout=timeout)

                if r.status_code == 200:
                    data = r.json()
                    choices = data.get("choices") or []
                    # OpenAI/Azure common shapes:
                    if choices and "message" in choices[0] and "content" in choices[0]["message"]:
                        if debug:
                            print(f"[llm_failover] {p['name']} succeeded (message.content).")
                        return choices[0]["message"]["content"]
                    if choices and "text" in choices[0]:
                        if debug:
                            print(f"[llm_failover] {p['name']} succeeded (text).")
                        return choices[0]["text"]
                    raise RuntimeError(f"{p['name']} unexpected response: {str(data)[:400]}")

                # Decide whether to fail over or retry this provider
                if _status_matches(failover_ranges, r.status_code):
                    last_err = RuntimeError(f"{p['name']} HTTP {r.status_code}: {r.text[:400]}")
                    if debug:
                        print(f"[llm_failover] Failover due to status {r.status_code} from {p['name']}")
                    break  # immediate failover to the next provider
                else:
                    last_err = RuntimeError(f"{p['name']} HTTP {r.status_code}: {r.text[:400]}")
                    if debug:
                        print(f"[llm_failover] Retry {p['name']} (attempt {attempt+1}/{retries}) after status {r.status_code}")
                    time.sleep(backoff * (attempt + 1))  # retry same provider
            except requests.RequestException as e:
                last_err = e
                if debug:
                    print(f"[llm_failover] Network error on {p['name']}: {e}. Retry {attempt+1}/{retries}")
                time.sleep(backoff * (attempt + 1))      # retry same provider
        # move to next provider after retries or failover trigger
        if debug:
            print(f"[llm_failover] Moving past {p['name']} to next provider...")

    # If we reach here, all providers failed
    if debug:
        print("[llm_failover] All providers failed.")
    raise last_err or RuntimeError("All LLM providers failed")


# ── Section 7: Example usage (optional) ───────────────────────────────────────
if __name__ == "__main__":
    # Minimal demo. Ensure you have either legacy API_URL/API_KEY (LLM_0)
    # or numbered providers (LLM_1_*, LLM_2_*, ...) in your environment.
    os.environ.setdefault("LLM_FAILOVER_DEBUG", "1")  # turn on debug prints for demo
    demo_messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user",   "content": "Say hi and tell me one fun fact."},
    ]
    try:
        print(chat_with_failover(demo_messages))
    except Exception as e:
        print("Failover demo error:", e)
